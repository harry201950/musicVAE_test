{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb","timestamp":1666677940178}],"collapsed_sections":["R122bwRNbTus"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"R122bwRNbTus"},"source":["# Basic Instructions\n","\n","1. Double click on the hidden cells to make them visible, or select \"View > Expand Sections\" in the menu at the top.\n","2. Hover over the \"`[ ]`\" in the top-left corner of each cell and click on the \"Play\" button to run it, in order.\n","3. Listen to the generated samples.\n","4. Make it your own: copy the notebook, modify the code, train your own models, upload your own MIDI, etc.!"]},{"cell_type":"markdown","metadata":{"id":"ZLfb2a_12wcj"},"source":["# 환경 설정\n","주피터노트북 환경에서 apt-get 커멘드가 동작 하지않아 코렙 환경에서 Colab Notebook Pre-trained Models 을 참조\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNfwlGL_v6ii","executionInfo":{"status":"ok","timestamp":1666792054827,"user_tz":-540,"elapsed":7385,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}},"outputId":"995655ad-b638-45e5-f4cd-a9ac17efd6a7"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"PfRDVhNs3UFx","executionInfo":{"status":"ok","timestamp":1666788306117,"user_tz":-540,"elapsed":407,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}}},"source":["'''!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n","!pip install -q pyfluidsynth\n","!pip install magenta==2.1.0\n","!pip install -qU magenta'''\n","\n","from google.colab import files\n","import ctypes.util\n","\n","import glob\n","import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow.compat.v1 as tf\n","import magenta.music as mm\n","import collections\n","import note_seq\n","import h5py\n","\n","from magenta.models.music_vae.trained_model import TrainedModel\n","from magenta.scripts.convert_dir_to_note_sequences import convert_directory\n","import magenta.models.music_vae as musicvae\n","from magenta.models.music_vae import configs\n","from magenta.common import merge_hparams\n","from magenta.models.music_vae import data\n","from magenta.models.music_vae import data_hierarchical\n","from magenta.models.music_vae import lstm_models\n","from magenta.models.music_vae.base_model import MusicVAE"],"execution_count":26,"outputs":[]},{"cell_type":"code","source":["orig_ctypes_util_find_library = ctypes.util.find_library\n","def proxy_find_library(lib):\n","  if lib == 'fluidsynth':\n","    return 'libfluidsynth.so.1'\n","  else:\n","    return orig_ctypes_util_find_library(lib)\n","ctypes.util.find_library = proxy_find_library\n","\n","\n","print('Importing libraries and defining some helper functions...')\n","\n","\n","tf.disable_v2_behavior()\n","\n","# Necessary until pyfluidsynth is updated (>1.2.5).\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","def play(note_sequence):\n","  mm.play_sequence(note_sequence, synth=mm.fluidsynth)\n","\n","def interpolate(model, start_seq, end_seq, num_steps, max_length=32,\n","                assert_same_length=True, temperature=0.5,\n","                individual_duration=4.0):\n","  \"\"\"Interpolates between a start and end sequence.\"\"\"\n","  note_sequences = model.interpolate(\n","      start_seq, end_seq,num_steps=num_steps, length=max_length,\n","      temperature=temperature,\n","      assert_same_length=assert_same_length)\n","\n","  print('Start Seq Reconstruction')\n","  play(note_sequences[0])\n","  print('End Seq Reconstruction')\n","  play(note_sequences[-1])\n","  print('Mean Sequence')\n","  play(note_sequences[num_steps // 2])\n","  print('Start -> End Interpolation')\n","  interp_seq = mm.sequences_lib.concatenate_sequences(\n","      note_sequences, [individual_duration] * len(note_sequences))\n","  play(interp_seq)\n","  mm.plot_sequence(interp_seq)\n","  return interp_seq if num_steps > 3 else note_sequences[num_steps // 2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52caVtXlr-1i","executionInfo":{"status":"ok","timestamp":1666785000956,"user_tz":-540,"elapsed":512,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}},"outputId":"a60b35f8-df91-4f81-f87c-4e8cd920aeea"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]},{"output_type":"stream","name":"stdout","text":["Importing libraries and defining some helper functions...\n"]}]},{"cell_type":"code","source":["'''Project Attributes'''\n","\n","midi_data = \"./drive/MyDrive/Colab Notebooks/musicVAE/groove\" #data file root\n","midi_dir = \"./drive/MyDrive/Colab Notebooks/musicVAE/groove/info.csv\" #csv file root\n","tfrecord_root = \"./drive/MyDrive/Colab Notebooks/musicVAE/music_tfrecord\"# tfrecord root\n","check_point_dir = \"./drive/MyDrive/Colab Notebooks/musicVAE/checkpoints/\" #check_point root\n","BASE_DIR = \"gs://download.magenta.tensorflow.org/models/music_vae/colab2\" #2_check_point root\n","musicvae_model_name = 'cat-drums_2bar_small' # pretrained MusicVAE model\n","data_path = './drive/MyDrive/Colab Notebooks/musicVAE/music_tfrecord'"],"metadata":{"id":"WuZHOJstQHzz","executionInfo":{"status":"ok","timestamp":1666794811131,"user_tz":-540,"elapsed":407,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["HParams = contrib_training.HParams\n","\n","\n","class Config(collections.namedtuple(\n","    'Config',\n","    ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n","     'train_examples_path', 'eval_examples_path', 'tfds_name'])):\n","\n","  def values(self):\n","    return self._asdict()\n","\n","Config.__new__.__defaults__ = (None,) * len(Config._fields)\n","\n","\n","def update_config(config, update_dict):\n","  config_dict = config.values()\n","  config_dict.update(update_dict)\n","  return Config(**config_dict)\n","\n","\n","CONFIG_MAP = {}\n","\n","\n","# Melody\n","CONFIG_MAP['cat-mel_2bar_small'] = Config(\n","    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n","                   lstm_models.CategoricalLstmDecoder()),\n","    hparams=merge_hparams(\n","        lstm_models.get_default_hparams(),\n","        HParams(\n","            batch_size=512,\n","            max_seq_len=32,  # 2 bars w/ 16 steps per bar\n","            z_size=256,\n","            enc_rnn_size=[512],\n","            dec_rnn_size=[256, 256],\n","            free_bits=0,\n","            max_beta=0.2,\n","            beta_rate=0.99999,\n","            sampling_schedule='inverse_sigmoid',\n","            sampling_rate=1000,\n","        )),\n","    note_sequence_augmenter=data.NoteSequenceAugmenter(transpose_range=(-5, 5)),\n","    data_converter=data.OneHotMelodyConverter(\n","        valid_programs=data.MEL_PROGRAMS,\n","        skip_polyphony=False,\n","        max_bars=100,  # Truncate long melodies before slicing.\n","        slice_bars=2,\n","        steps_per_quarter=4),\n","    train_examples_path='./drive/MyDrive/Colab Notebooks/musicVAE/music_tfrecord',\n","    eval_examples_path=None,\n",")"],"metadata":{"id":"xr6h5Z1Yq8iA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data load\n","df = pd.read_csv(midi_dir)\n","df = pd.DataFrame(df)\n","df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"YvHwzt7rOOSv","executionInfo":{"status":"ok","timestamp":1666785647196,"user_tz":-540,"elapsed":6,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}},"outputId":"4fd08853-72e4-4be2-d1a4-912cf32a8ebe"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    drummer                session                        id          style  \\\n","0  drummer1  drummer1/eval_session   drummer1/eval_session/1   funk/groove1   \n","1  drummer1  drummer1/eval_session  drummer1/eval_session/10  soul/groove10   \n","2  drummer1  drummer1/eval_session   drummer1/eval_session/2   funk/groove2   \n","3  drummer1  drummer1/eval_session   drummer1/eval_session/3   soul/groove3   \n","4  drummer1  drummer1/eval_session   drummer1/eval_session/4   soul/groove4   \n","\n","   bpm beat_type time_signature  \\\n","0  138      beat            4-4   \n","1  102      beat            4-4   \n","2  105      beat            4-4   \n","3   86      beat            4-4   \n","4   80      beat            4-4   \n","\n","                                       midi_filename  \\\n","0  drummer1/eval_session/1_funk-groove1_138_beat_...   \n","1  drummer1/eval_session/10_soul-groove10_102_bea...   \n","2  drummer1/eval_session/2_funk-groove2_105_beat_...   \n","3  drummer1/eval_session/3_soul-groove3_86_beat_4...   \n","4  drummer1/eval_session/4_soul-groove4_80_beat_4...   \n","\n","                                      audio_filename   duration split  \n","0  drummer1/eval_session/1_funk-groove1_138_beat_...  27.872308  test  \n","1  drummer1/eval_session/10_soul-groove10_102_bea...  37.691158  test  \n","2  drummer1/eval_session/2_funk-groove2_105_beat_...  36.351218  test  \n","3  drummer1/eval_session/3_soul-groove3_86_beat_4...  44.716543  test  \n","4  drummer1/eval_session/4_soul-groove4_80_beat_4...  47.987500  test  "],"text/html":["\n","  <div id=\"df-c770665f-2ec8-491c-b7c4-771a06e7d106\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>drummer</th>\n","      <th>session</th>\n","      <th>id</th>\n","      <th>style</th>\n","      <th>bpm</th>\n","      <th>beat_type</th>\n","      <th>time_signature</th>\n","      <th>midi_filename</th>\n","      <th>audio_filename</th>\n","      <th>duration</th>\n","      <th>split</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>drummer1</td>\n","      <td>drummer1/eval_session</td>\n","      <td>drummer1/eval_session/1</td>\n","      <td>funk/groove1</td>\n","      <td>138</td>\n","      <td>beat</td>\n","      <td>4-4</td>\n","      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n","      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n","      <td>27.872308</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>drummer1</td>\n","      <td>drummer1/eval_session</td>\n","      <td>drummer1/eval_session/10</td>\n","      <td>soul/groove10</td>\n","      <td>102</td>\n","      <td>beat</td>\n","      <td>4-4</td>\n","      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n","      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n","      <td>37.691158</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>drummer1</td>\n","      <td>drummer1/eval_session</td>\n","      <td>drummer1/eval_session/2</td>\n","      <td>funk/groove2</td>\n","      <td>105</td>\n","      <td>beat</td>\n","      <td>4-4</td>\n","      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n","      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n","      <td>36.351218</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>drummer1</td>\n","      <td>drummer1/eval_session</td>\n","      <td>drummer1/eval_session/3</td>\n","      <td>soul/groove3</td>\n","      <td>86</td>\n","      <td>beat</td>\n","      <td>4-4</td>\n","      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n","      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n","      <td>44.716543</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>drummer1</td>\n","      <td>drummer1/eval_session</td>\n","      <td>drummer1/eval_session/4</td>\n","      <td>soul/groove4</td>\n","      <td>80</td>\n","      <td>beat</td>\n","      <td>4-4</td>\n","      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n","      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n","      <td>47.987500</td>\n","      <td>test</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c770665f-2ec8-491c-b7c4-771a06e7d106')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c770665f-2ec8-491c-b7c4-771a06e7d106 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c770665f-2ec8-491c-b7c4-771a06e7d106');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["convert_directory(midi_data,tfrecord_root, True)"],"metadata":{"id":"SqmbZVUtQ9qM","executionInfo":{"status":"ok","timestamp":1666776212411,"user_tz":-540,"elapsed":732,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["'''\n","@https://github.com/maxwells-daemons/accompany-music-vae/blob/master/data_utils/tfrecord_to_hdf5.py\n","Turn a tfrecord of NoteSequences into an HDF5 dataset with instruments split\n","and the pretrained trio mode's latent vectors.\n","'''\n","\n","from copy import deepcopy\n","from time import time\n","import itertools as it\n","\n","import click\n","import logging\n","from pprint import pformat\n","\n","import numpy as np\n","import h5py\n","\n","import tensorflow as tf\n","tf.logging.set_verbosity(tf.logging.ERROR)  # noqa\n","\n","import magenta.music as mm\n","from magenta.models.music_vae import configs\n","from magenta.models.music_vae.trained_model import TrainedModel\n","\n","from constants import (TIMESTEPS, DIM_MELODY, DIM_BASS, DIM_DRUMS, DIM_TRIO)\n","\n","# Constants\n","MODEL_NAME = 'cat-drums_2bar_small'\n","\n","\n","@click.command()\n","@click.argument('input_file', type=click.Path(exists=True))\n","@click.argument('output_file', type=click.Path(exists=False))\n","@click.option('--include_all_instruments', type=bool, default=False)\n","@click.option('--chunk_size', type=click.IntRange(min=1), default=128,\n","              help='Number of MIDI files to read at once.')\n","@click.option('--buffer_size', type=click.IntRange(min=1), default=50000,\n","              help='Number of examples to make room for at a time.')\n","@click.option('--batch_size', type=click.IntRange(min=1), default=256,\n","              help='Batch size for the pretrained model.')\n","@click.option('--checkpoint', type=click.Path(),\n","              default='./models/pretrained/{}.ckpt'.format(MODEL_NAME),\n","              help='Checkpoint to use for the pretrained model.')\n","@click.option('--log_period', type=click.IntRange(min=0), default=1,\n","              help='How many chunks pass between logging lines.')\n","@click.option('--log_file', type=click.Path(),\n","              default='logs/split_dataset.log')\n","def main(input_file, output_file,\n","         include_all_instruments, chunk_size, buffer_size, batch_size,\n","         checkpoint, log_period, log_file):\n","    args = locals()\n","    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n","    log = logging.getLogger(__name__)\n","    handler = logging.FileHandler(log_file)\n","    handler.setFormatter(formatter)\n","    handler.setLevel(logging.DEBUG)\n","    log.addHandler(handler)\n","    log.setLevel(logging.DEBUG)\n","\n","    log.info('Generating melody dataset with args:\\n' + pformat(args))\n","    total_start_time = time()\n","    ns_gen = mm.note_sequence_io.note_sequence_record_iterator(input_file)\n","    ns_iter = iter(ns_gen)\n","    config = configs.CONFIG_MAP[MODEL_NAME]\n","    trio_converter = config.data_converter\n","\n","    log.debug('Creating HDF5 store...')\n","    start_time = time()\n","    with h5py.File(output_file, 'w') as data_file:\n","        dataset_size = buffer_size\n","        ds_melody = data_file.create_dataset(\n","            'melody',\n","            (dataset_size, TIMESTEPS, DIM_MELODY),\n","            maxshape=(None, TIMESTEPS, DIM_MELODY),\n","            dtype=np.bool\n","        )\n","        ds_code = data_file.create_dataset(\n","            'code',\n","            (dataset_size, config.hparams.z_size),\n","            maxshape=(None, config.hparams.z_size),\n","            dtype=np.float32\n","        )\n","\n","        if include_all_instruments:\n","            ds_trio = data_file.create_dataset(\n","                'trio',\n","                (dataset_size, TIMESTEPS, DIM_TRIO),\n","                maxshape=(None, TIMESTEPS, DIM_TRIO),\n","                dtype=np.bool\n","            )\n","            ds_bass = data_file.create_dataset(\n","                'bass',\n","                (dataset_size, TIMESTEPS, DIM_BASS),\n","                maxshape=(None, TIMESTEPS, DIM_BASS),\n","                dtype=np.bool\n","            )\n","            ds_drums = data_file.create_dataset(\n","                'drums',\n","                (dataset_size, TIMESTEPS, DIM_DRUMS),\n","                maxshape=(None, TIMESTEPS, DIM_DRUMS),\n","                dtype=np.bool\n","            )\n","\n","        log.debug('Done creating HDF5 store (time: {0:.1f}s)'\n","                  .format(time() - start_time))\n","\n","        log.debug('Loading model...')\n","        start_time = time()\n","        model = TrainedModel(config, batch_size=batch_size,\n","                             checkpoint_dir_or_path=checkpoint)\n","        log.debug('Done loading model (time: {0:.1f}s)'\n","                  .format(time() - start_time))\n","\n","        log.info('Beginning dataset creation...')\n","        i_chunk = 0\n","        i_example = 0\n","        try:\n","            while True:\n","                i_chunk += 1\n","                log.disabled = i_chunk % log_period != 0 or not log_period\n","                chunk_time = time()\n","\n","                log.debug('Processing a chunk of NoteSequences...')\n","                start_time = time()\n","\n","                note_sequences = list(it.islice(ns_iter, chunk_size))\n","                if not note_sequences:\n","                    break\n","\n","                trio_tensors = map(\n","                    lambda seq: trio_converter.to_tensors(seq).outputs,\n","                    note_sequences\n","                )\n","                trio_tensors = it.chain.from_iterable(trio_tensors)\n","                trio_tensors = list(\n","                    filter(lambda t: t.shape == (TIMESTEPS, DIM_TRIO),\n","                           trio_tensors)\n","                )\n","\n","                # Ensure an example doesn't overflow the allocated space\n","                trio_tensors = trio_tensors[:buffer_size]\n","                n_tensors = len(trio_tensors)\n","                i_last = n_tensors + i_example\n","\n","                melody_tensors = list(map(lambda t: t[:, :DIM_MELODY],\n","                                          trio_tensors))\n","\n","                if include_all_instruments:\n","                    bass_tensors = list(map(\n","                        lambda t: t[:, DIM_MELODY:DIM_MELODY + DIM_BASS],\n","                        trio_tensors\n","                    ))\n","                    drums_tensors = list(map(lambda t: t[:, -DIM_DRUMS:],\n","                                             trio_tensors))\n","\n","                log.debug('Done processing NoteSequences (time: {0:.1f}s)'\n","                          .format(time() - start_time))\n","\n","                log.debug('Running encoder...')\n","                start_time = time()\n","                _, codes, _ = model.encode_tensors(deepcopy(trio_tensors),\n","                                                   [TIMESTEPS] * n_tensors)\n","                log.debug('Done running encoder (time: {0:.1f}s)'\n","                          .format(time() - start_time))\n","\n","                if i_last >= dataset_size:\n","                    dataset_size += buffer_size\n","                    log.info('Resizing datasets to size:', dataset_size)\n","                    ds_melody.resize((dataset_size, TIMESTEPS, DIM_MELODY))\n","                    ds_code.resize((dataset_size, config.hparams.z_size))\n","\n","                    if include_all_instruments:\n","                        ds_trio.resize((dataset_size, TIMESTEPS, DIM_TRIO))\n","                        ds_bass.resize((dataset_size, TIMESTEPS, DIM_BASS))\n","                        ds_drums.resize((dataset_size, TIMESTEPS, DIM_DRUMS))\n","\n","                log.debug('Writing examples to HDF5...')\n","                start_time = time()\n","                ds_melody[i_example:i_last, :, :] = np.array(melody_tensors)\n","                ds_code[i_example:i_last, :] = np.array(codes)\n","\n","                if include_all_instruments:\n","                    ds_trio[i_example:i_last, :, :] = np.array(trio_tensors)\n","                    ds_bass[i_example:i_last, :, :] = np.array(bass_tensors)\n","                    ds_drums[i_example:i_last, :, :] = np.array(drums_tensors)\n","\n","                log.debug('Done writing examples to HDF5 (time: {0:.1f}s)'\n","                          .format(time() - start_time))\n","\n","                i_example += n_tensors\n","\n","                log.info(('Chunk {0} wrote {1} examples ' +\n","                         '(total: {2}; time: {3:.1f}s)')\n","                         .format(i_chunk, n_tensors, i_example,\n","                                 time() - chunk_time))\n","        except StopIteration:\n","            pass\n","\n","    log.debug('Finished writing data')\n","    log.debug('Resizing datasets...')\n","    dataset_size = i_example\n","    ds_melody.resize((dataset_size, TIMESTEPS, DIM_MELODY))\n","    ds_code.resize((dataset_size, config.hparams.z_size))\n","    if include_all_instruments:\n","        ds_trio.resize((dataset_size, TIMESTEPS, DIM_TRIO))\n","        ds_bass.resize((dataset_size, TIMESTEPS, DIM_BASS))\n","        ds_drums.resize((dataset_size, TIMESTEPS, DIM_DRUMS))\n","    log.debug('Done resizing datasets...')\n","\n","    total_time = time() - total_start_time\n","    log.info('Finished creating HDF5 dataset')\n","    log.info('Total examples: {}'.format(i_example))\n","    log.info('Total chunks: {}'.format(i_chunk))\n","    log.info('Total time: {0:.1f}s'.format(total_time))\n","    log.info('Done!')\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"R304YcBax_uX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keras.layers\n","import keras.models\n","\n","def get_model(name='encoder', optimizer=None):\n","    '''\n","    Get the compiled surrogate encoder model.\n","    Parameters\n","    ----------\n","    name : str (option)\n","        The model name.\n","    optimizer : default rmsprop optimizer.\n","    '''\n","    input_layer = keras.layers.Input(shape=(256, 90), name='input')\n","    for i in range(2):\n","        layer = keras.layers.Bidirectional(\n","            keras.layers.LSTM(128,\n","                         return_sequences = True,\n","                         name='bi_lstm_{}'.format(i)))(layer)\n","    output_layer = keras.layers.Dense(512, activation='linear', name='output')(input_layer)\n","    model = keras.models.Model(inputs=input_layer, \n","                               outputs=output_layer)\n","\n","    optimizer = keras.optimizers.rmsprop(lr=0.0005, clipnorm=1.)\n","    model.compile(optimizer=optimizer, loss='mean_squared_error')\n","    model.name = name\n","\n","    return model\n","\n","    '''@출처 : https://github.com/maxwells-daemons/accompany-music-vae/blob/master/model.py'''"],"metadata":{"id":"KpE9qHh2by-I","executionInfo":{"status":"ok","timestamp":1666794528017,"user_tz":-540,"elapsed":405,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def train_model(model, batch_size, epochs, data_path):\n","    '''\n","    Train a surrogate encoder model.\n","    Parameters\n","    ----------\n","    model : None, keras Model, The model to train.\n","        If None, initializes a new default model.\n","    batch_size : int\n","        Batch size for training.\n","    epochs : int\n","        Number of epochs to train for.\n","    data_path : str path to hdf5\n","        Path to the file of training data.\n","    Raises\n","    ------\n","    AssertionError\n","        If model is a str but not a valid path.\n","    '''\n","\n","    if not model:\n","        model = get_model()\n","    elif isinstance(model, str):\n","        assert(os.path.exists(model))\n","        model = keras.load_model(model)\n","    # Otherwise, assume the model is a Keras model\n","\n","    # TODO: LambdaCallback to produce and save samples at each epoch\n","    checkpointer = keras.callbacks.ModelCheckpoint(\n","        os.path.join(\n","            check_point_dir,\n","            model.name + '_train_{epoch:02d}-{val_loss:.4f}.hdf5'\n","        ),\n","        save_best_only=False, verbose=1\n","    )\n","    callbacks = checkpointer\n","\n","    data_file = h5py.File(data_path, 'r')\n","    data_dir = os.path.dirname(data_path)\n","    train_seq = HDF5Sequence(\n","        data_file, batch_size,\n","        index_path=os.path.join(data_dir, 'train_indices.csv'))\n","    val_seq = HDF5Sequence(\n","        data_file, batch_size,\n","        index_path=os.path.join(data_dir, 'val_indices.csv'))\n","\n","    model.fit_generator(train_seq, steps_per_epoch=len(train_seq),\n","                        validation_data=val_seq, validation_steps=len(val_seq),\n","                        max_queue_size=128, workers=32, epochs=20,\n","                        callbacks=callbacks)\n","\n","\n","if __name__ == '__main__':\n","    train_model()"],"metadata":{"id":"WHNhrALRbzAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iVZt3TrQQKlX","executionInfo":{"status":"ok","timestamp":1666792821842,"user_tz":-540,"elapsed":716,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def generate_accompaniment(seq, surrogate_encoder, musicvae=None,\n","                           stitch=True, extract_melody=True,\n","                           remove_controls=True, temperature=0.1):\n","    '''\n","    Generate accompaniment for an input sequence.\n","    Parameters\n","    ----------\n","    seq : str path to midi or NoteSequence\n","        The input sequence.\n","    surrogate_encoder : keras Model\n","        The model to map melodies to latent vectors.\n","    musicvae : None or Magenta MusicVAE (optional)\n","        The MusicVAE to use for decoding.\n","        If None, loads the default MusicVAE.\n","        NOTE: For quickly performing inference on multiple input batches,\n","        preload the default MusicVAE outside this function and pass it in.\n","    stitch : bool (optional)\n","        Whether to stitch in the original melody or leave the decoded sequence.\n","    extract_melody : bool (optional)\n","        Whether to treat the input as a trio and extract the melody.\n","    remove_controls : bool (optional)\n","        Whether to delete tempo changes, time changes, etc from the base midi.\n","    temperature : float (optional)\n","        Temperature to use in the trio decoder.\n","    Returns\n","    -------\n","    NoteSequence\n","        The input sequence along with generated accompaniment.\n","    '''\n","\n","    config = configs.CONFIG_MAP['cat-mel_2bar_small']\n","    melody_converter = config.data_converter._melody_converter\n","\n","    musicvae = trained_model.TrainedModel(\n","        config, batch_size = 4,\n","        checkpoint_dir_or_path= os.join(\n","            BASE_DIR + musicvae_model_name + '.ckpt')\n","\n","    # If the sequence is provided as a MIDI path, load it\n","    if isinstance(seq, str):\n","        midi = None\n","        with open(seq, 'rb') as midi_file:\n","            midi = midi_file.read()\n","        seq = mm.midi_to_sequence_proto(midi)\n","\n","    if remove_controls:\n","        del seq.tempos[1:]\n","        del seq.time_signatures[1:]\n","        del seq.control_changes[1:]\n","\n","    if extract_melody:\n","        seq = strip_to_melody(seq)\n","\n","    # Convert the input NoteSequence to a single-instrument tensor\n","    melody_tracks = melody_converter.to_tensors(seq).outputs\n","    instrument_counts = [np.sum(melody_tracks[i][:, 1:])\n","                         for i in range(len(melody_tracks))]\n","    instrument_idx = np.argmax(instrument_counts)\n","    melody_tensor = melody_tracks[instrument_idx]\n","\n","    # Slice the melody into non-overlapping windows\n","    windows = [melody_tensor[i * TIMESTEPS:(i+1) * TIMESTEPS, :]\n","               for i in range(melody_tensor.shape[0] // TIMESTEPS + 1)]\n","    windows[-1] = np.pad(windows[-1],\n","                         [(0, max(0, TIMESTEPS - windows[-1].shape[0])),\n","                          (0, 0)],\n","                         mode='constant')\n","    windows_stacked = np.stack(windows)\n","\n","    # Perform inference\n","    latent_codes = surrogate_encoder.predict(windows_stacked)\n","    decoded_sequences = musicvae.decode(latent_codes, temperature=temperature)\n","    decoded = concatenate_sequences(decoded_sequences)\n","\n","    # Stitch the original melody and the new accompaniment together.\n","    if stitch:\n","        melody_tensor_padded = np.stack(windows_stacked, axis=0)\n","        melody_padded = concatenate_sequences(\n","            melody_converter.to_notesequences(melody_tensor_padded))\n","        out = remove_melody(decoded)\n","        out.MergeFrom(melody_padded)\n","        return out\n","\n","    return decoded"],"metadata":{"id":"1wShgJywW-kW","executionInfo":{"status":"ok","timestamp":1666794260286,"user_tz":-540,"elapsed":514,"user":{"displayName":"YoungBok Hong","userId":"00711479927724396415"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hhcvaCNAaEI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Hde0VPY0aELd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ArLNRlJOW-nS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4pRTwQkkW-qD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LLTjVb6bVoZ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r56hPjGtUx4Z"},"execution_count":null,"outputs":[]}]}